# -*- coding: utf-8 -*-
"""Big Data Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ILvsnCGSlUWjcT-GUfVCFHT4893av25a
"""

!pip install pyspark

from pyspark import SparkContext
sc = SparkContext.getOrCreate()

rdd_file = sc.textFile('/content/fixed_year_2020_quarter_01.csv')

for element in rdd_file.collect():
  print(element)

rdd_filter_A = rdd_file.filter(lambda x : 'A' in x[0])
for element in rdd_filter_A.collect():
  print(element)

rdd_filter_B = rdd_file.filter(lambda x : 'B' in x[0])
for element in rdd_filter_B.collect():
  print(element)

rdd_filter_A = rdd_file.filter(lambda x : 'A' in x[0])
rdd_filter_B = rdd_file.filter(lambda x : 'B' in x[0])

rdd_union = rdd_filter_A.union(rdd_filter_B)
for element in rdd_union.collect():
  print(element)

print("No. of rows of Rdd file : " +str(rdd_file.count()) + " rows")

rdd_filter_G = rdd_file.filter(lambda x : 'G' in x[0])
for element in rdd_filter_G.collect():
  print(element)

rdd_filter_G = rdd_file.filter(lambda x : 'G' in x[0])
print("No. of rows starts with G : " +str(rdd_filter_G.count()) + " rows")

import pandas as pd

from pyspark.sql import SparkSession
from pyspark.context import SparkContext
import time

from pyspark.sql import SparkSession

spark = (SparkSession
  .builder
  .appName("SparkSQL")
  .getOrCreate())

dataframe_csv = "/content/fixed_year_2020_quarter_01.csv"

df = (spark.read.format("csv")
  .option("inferSchema", "true")
  .option("header", "true")
  .load(dataframe_csv))

print(df)